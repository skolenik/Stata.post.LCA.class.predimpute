\inserttype[st0001]{article}
\author{Kolenikov}{%
  Stas Kolenikov\\NORC\\Columbia, Missouri, USA\\kolenikov-stas@norc.org
}
\title[Post-estimation for LCA via MI]{Inference for imputed latent classes using multiple imputation}
\maketitle

\begin{abstract}
I introduce a command to multiply impute latent classes
following \stcmd{gsem, lclass()} latent class analysis. 
This allows properly propagating uncertainty in class 
membership to downstream analysis that may characterize
the demographic composition of the classes, or use 
the class as a predictor variable in statitsical models.

\keywords{\inserttag, postlca\_class\_predpute, latent class analysis, multiple imputation}
\end{abstract}

\section{Latent class analysis}

Latent class analysis (LCA) is a commonly used statistical and quantitative
social science technique of modeling counts in high dimensional contingency tables,
or tables of associations of categorical variables
\citet{hagenaars:mccutcheon:2002,mccutcheon:1987}. 
LCA is a form of loglinear modeling, so let us explain that first.
If the researcher has several categorical variables $X_1, X_2, \ldots, X_p$
with categories 1 through $m_j, j=1, \ldots, p$,
at their disposal, and can produce counts $n_{{k_1}{k_2}\ldots{k_p}}$ 
in a complete $p$-dimensional table, the first step could be modeling
in main effects:

\begin{equation}
  \label{eq:loglinear:main}
  \log \Expect n_{{k_1}{k_2}\ldots{k_p}} = 
  \mathrm{offset} + 
  \sum_{j=1}^p \sum_{k=1}^{m_j} \beta_{jk_j} 
\end{equation}

\noindent
with applicable identification constraints (such as the sum of the coefficients
of a single variable is zero, or the coefficient for the first category 
of a variable is zero). Parameter estimates can be obtained by maximum
likelihood, as equation (\ref{eq:loglinear:main}) is a Poisson regression model.
This model can be denoted as $X_1 + X_2 + \ldots + X_p$ main effects model.
The fit of the model is assessed by the Pearson $\chi^2$ test comparing the
expected vs. observed cell counts, or the likelihood ratio test against a saturated
model where each cell in the full $p$-dimensional table
has its own coefficient. If the main effects model were to be found inadequate,
the researcher can entertain adding interactions, e.g. the interaction of 
$X_1$ and $X_2$ would have $m_1 \times m_2$ terms for each pair of values 
of these variables, rather than $m_1 + m_2$ main effects, as well as 
remaining $p-2$ variables in their main effects form:

\begin{equation}
  \label{eq:loglinear:x1x2}
  \log \Expect n_{{k_1}{k_2}\ldots{k_p}} = 
  \mathrm{offset} + 
  \sum_{k_1=1}^{m_1} \sum_{k_2=1}^{m_2} \beta_{12,k_1k_2} +
  \sum_{j=3}^p \sum_{k=1}^{m_j} \beta_{jk_j} 
\end{equation}

This model, in the notation that is closer to Stata interaction terms
than the traditional exposition of LCA, 
can be denoted as $X_1 \# X_2 + X_3 + \ldots + X_p$.

In the loglinear model notation, the latent class models 
are models of the form $C \# (X_1 + X_2 + \ldots + X_p)$.
Categorical latent variable $C$ is the latent class.
The model is now a mixture of Poisson regressions,
and maximum likelihood estimation additionally involves 
estimating the prevalence of each class of $C$.

Further extensions of latent class analysis may include:

\begin{enumerate}
  \item Analysis with interactions of the observed variables;
  \item Analysis with complex survey data (in which case estimation proceeds
      with \stcmd{svy} prefix, and the counts are the weighted estimates 
      of the population totals in cells);
  \item Constrained analyses with structural zeroes or ones 
      (e.g. that every member of class $C=1$ must have the value
      $X_1=1$, expressed as the corresponding coefficient 
      $\beta_{11\cdot}=-\infty$ for all categories except 1);
  \item Constrained analyses (measurement invariance) where some 
      variables or interations have identical
      coefficients across classes.
\end{enumerate}

\subsection{Official Stata implementation}

Official Stata \stcmd{gsem, lclass()} implements 
the \textit{main effects} LCA. The syntax is that of the SEM
families, with the variables that the arrow points to
interpreted as the outcome variables, and the latent class
variable considered the source of the arrow:

\begin{stlog}
. webuse gsem_lca1
\smallskip
. gsem (accident play insurance stock <- ), logit lclass(C 2)
\end{stlog}

The goodness of fit test against the free tabulation counts
is provided by \stcmd{estat gof} (not available after
the complex survey data analysis.)

As LCA is implemented through \stcmd{gsem},
all the link functions and generalized linear model families
are supported, extending the ``mainstream'' LCA to include
multinomial and ordinal variables. (When all outcomes are continuous,
the model is referred to as \textit{profile analysis}.)

\subsection{Examples }

LCA has found use in analyses of complicated economic concepts
from survey data, and in assessment of measurement errors
that arise in the process.

\citet{biemer:2004} provided analysis of labor force classification
status (employed, unemployed, and out of labor force) following changes
in the survey instrument used in the Current Population Survey (CPS).
The latent classes are the true 
LFS categories, and the observed variables are the corresponding 
survey measurements, demographics, and survey interview mode.
The model is a variation of LCA that accounts for the survey methodology
aspects of CPS: 
its panel nature (responses are collected over four consecutive months) 
and response mode (proxy reporting when a family member provides
the survey responses rather than the target person.)
He found that measurement of being employed and not being in labor force
are highly accurate (98\% and 97\% accuracy) while measurement of 
being unemployed is much less accurate (between 74\% and 81\% depending
on the analysis year.) LCA allowed to further attribute the drop in
accuracy of the unemployment status measurement to proxy reporting,
and to the problems with measuring the employment status when 
the worker is laid off.

\citet{kolenikov:daley:2017} analyzed the latent classes
of employees 
using the U.S. Department of Labor Worker Classification Survey.
The observed variables were (composite) 
self-report of the employment
status (are you an employee at your job; do you refer to your work
as your business, your client, your job, etc.); 
tax status
(the forms that the worker receives from their firm: W-2, 1099, K-1, etc.);
behavioral control 
(functions the worker performs and the degree of control over these functions,
such as direct reporting to somebody, schedule, permission to leave, etc.);
and non-control composite
(hired for fixed time or specific project). They found the best fitting
model to contain three classes: employees-and-they-know-it (59\%),
nonemployees-and-they-know-it (24\%), and confused (17\%) who
classify themselves as employees but their tax documentation
is unclear, and other variables tend to place them into non-employee status.

\subsection{Scope for this package}

Researchers are often interested in describing the latent classes
or using these classes in analysis as predictors or as moderators.
The official \semref{gsem postestimation} commands provide
limited possibilities, namely reporting of the means
of the dependent variables by class via \stcmd{estat lcmean}.
For nearly all meaningful applications of LCA, this is insufficient.

The program distributed with the current package,
\stcmd{postlca\_class\_predpute}, provides a pathway for the appropriate
statistical inference that would account for uncertainty in class prediction.
This is achieved through the mechanics of multiple imputation
\citep{vanbuuren:2018:fimd2}. 
The name is supposed to convey that
\begin{enumerate}
  \item it is run after LCA as a post-estimation command;
  \item it predicts / imputes the latent classes.
\end{enumerate}

\section{The new command}

Imputation of latent classes, a \stcmd{gsem} postestimation command:

\begin{stsyntax}
    postlca\_class\_predpute,
    lcimpute(\varname)
    addm(\num)
    \optional{ seed(\num) }
\end{stsyntax}

\hangpara{
  \stcmd{lcimpute(}\varname\stcmd{)} 
  specifies the name of the latent class variable to be imputed.
  This option is required.
}

\hangpara{
  \stcmd{addm(}\num{)} specifies the number of imputations to be created.
  This option is required.
}

\hangpara{
  \stcmd{seed(}\num\stcmd{)} specifies the random number seed.
}

\section{Examples}

\subsection{Stata manual data set example}

The LCA capabilities of Stata are exemplified in [SEM] \textbf{Example 50g}:

\noindent
\begin{stlog}
\input{example_lca1_edited.log.tex}\nullskip
\end{stlog}

The official post-estimation commands available after
\stcmd{gsem, lclass()} produce the estimated class probabilities
and class-specific means of the outcome variables used in the model:

\noindent
\begin{stlog}
\input{example_lca1_lcamean.log.tex}\nullskip
\end{stlog}

The mutiple imputation version of this estimation task could
look as follows:

\begin{stlog}
\input{example_lca1_predpute.log.tex}\nullskip
\end{stlog}

The name of the latent class variable (here, \stcmd{lclass})
and the number of imputations are required. The seed is optional,
but of course is strongly recommended for reproducibility of the results,
as the underlying data are randomly simulated.
The multiple imputation version is notably faster.

As one of many diagnostic outputs of MI, the increase in variances / standard errors
due to imputations serves as an indication of how much of a problem
would treating the singly imputed (e.g. modal probability) latent classes would have been.
In the above output, the fraction of missing data (FMI)
is 33\% to 40\%, and the relative variance increase (RVI) is the similar range 
from 39\% to 45\%. This means that the analysis with the deterministic
(modal) imputation of the classes would have had standard errors 
that are about 20\% too small. (In fact, the model has a structural zero,
and modal imputation struggles to produce any standard errors around
the estimate, at all.)

\begin{stlog}
\input{example_lca1_modal.log.tex}\nullskip
\end{stlog}


\subsection{NHANES complex survey data example}

In many important and realistic applications of LCA, including the case
that necessitated the development of this package
\citep{rowan:etal:2024}, the data come from
complex survey designs that require setting the data up for the appropriate
survey-design adjusted analyses. See \svyref{svyset}, \miref{mi svyset},
and \citet{kolenikov:pitblado:2014}.

The standard data set for the \svyref{} commands is an extract from 
the National Health and Nutrition Examination Survey, Round Two
(NHANES II) data. I will use a handful of binary health outcomes
and one ordinal outcome to demonstrate LCA; the ordinal outcome
is arguably an extension that is not quite well covered in the
``classical'' social science LCA.

\begin{stlog}
\input{nhanes2.log.tex}\nullskip
\end{stlog}

This analysis approximates breaking down the population
into "generally healthy" and "unhealthy" groups, as e.g.
the gradient of \textit{hlthstat} variable between the classes
shows. The official \stcmd{gsem postestimation} commands
take approximately forever to run.
Some complicated interaction of \stcmd{svy} and \stcmd{gsem} 
made the default starting infeasible, 
hence I had to specify the initial random search.
The use of the \stcmd{postlca\_class\_predpute} command
makes it possible to run the analylsis much faster
(although that has never been the intent of the package),
and to conduct complementary analyses,
e.g. analysis of the racial composition of the two classes
using a variable that it outside of the model.

\begin{stlog}
\input{example_nhanes2_predpute.log}\nullskip
\end{stlog}

The less healthy class has a higher concentration
of ethnic minorities.

\subsection{Choosing the number of imputations}

One ``researcher's degrees of freedom'' aspect of this analysis
is the number of imputations $M$ that need to be created.
What this number affects the most is the stability of the standard
errors obtained through the multiple imputation process.
This stability is internally assessed with estimated
degrees of freedom associated with the variance estimate
\citep{barnard:rubin:1999}. With $M=10$ imputations,
the smaller ``poor health'' class have about 50 degrees of freedom:

\begin{stlog}
\input{example_nhanes2_dftable10.log}\nullskip
\end{stlog}

The public use version of the NHANES II data uses the approximate
design that has 62 PSU in 31 strata, resulting in 31 design degrees 
of freedom. The imputation degrees of freedom barely exceed that.
Let us push the number of imputations up (I used $M=62$ to match
the number of PSUs):

\begin{stlog}
\input{example_nhanes2_dftable62.log}\nullskip
\end{stlog}

The MI degrees of freedom are now comfortably above 600.
In many i.i.d. data situations, increasing the number of imputations
to several dozens can often send the MI degrees of freedom to 
approximate infinity (the reported numbers are in hundreds 
of thousands). With complex survey designs that have limited
degrees of freedom within each implicate, this may not materialize.
Researchers are encouraged to adopt the workflow where, in parallel,
they

\begin{enumerate}
  \item start with a small number of imputations, like \stcmd{addm(10)}
        in the example above, and develop the analysis code for all
        the substantive analyses, and
  \item working with the key outcomes or analyses, 
        experiment with several values of $M$ to find a reasonable
        trade-off when degrees of freedom exceed the sample size 
        for i.i.d. data, and/or exceed the design degrees of freedom
        for complex survey data by a factor of 3--5.
\end{enumerate}

Then a chosen value of $M$ can be used for all analyses in the paper.

An additional consideration for the choice of the number of imputations
is ensuring diversity of the simulated classes.
Even a large number of replications may not protect the researcher
from classes that may have structural zeroes. These produce
zero standard errors and missing degrees of freedom and 
variance increase statistics:

\begin{stlog}
\input{example_nhanes2_dftable62gh.log}\nullskip
\end{stlog}

\section{Simulation}

So how much of a problem is the modal imputation of the latent classes?
I ran a small simulation to investigate. Samples were taken from
a model with three classes, five binary indicators and one additional 
continuous variable not used in the model as follows:

\begin{tabular}{l|ccc}
   \noalign{\smallskip}
  \hline
  \noalign{\smallskip}
  Variable & Class 1 & Class 2 & Class 3 \\
  \noalign{\smallskip}
  \hline
  \noalign{\smallskip}
  Class probability & 0.4 & 0.4 & 0.2 \\
  $\mathbb{P}[y_1=1|C]$ & 0.7 & 0.3 & 0.6 \\
  $\mathbb{P}[y_2=1|C]$ & 0.8 & 0.5 & 0.6 \\
  $\mathbb{P}[y_3=1|C]$ & 0.5 & 0.4 & 0.7 \\
  $\mathbb{P}[y_4=1|C]$ & 0.5 & 0.3 & 0.7 \\
  $\mathbb{P}[y_5=1|C]$ & 0.8 & 0.4 & 0.3 \\
  \noalign{\smallskip}
  \hline
  \noalign{\smallskip}  
  $y_6\sim N(\mu_c,1)$ & 1 & $\sqrt{2}=1.41$ & $\sqrt{3}=1.73$ \\
  \noalign{\smallskip}
  \hline
  \noalign{\smallskip}  
\end{tabular}

The LCA model with outcomes $y_1$ through $y_5$ was estimated,
and the classes were predicted using the posterior modal prediction,
multiple imputation with $M=5$ imputed data sets, and $M=50$ imputed 
data sets.
(To be precise, there was a single imputation with $M=50$, but 
two versions were run: \stcmd{mi estimate , imp(1/5)} for the limited
application with $M=5$, and without the \stcmd{imp()} option for the full
set of $M=50$.) Inspecting the individual runs visually, a shorter set
of imputations often results in insufficient detail, namely failing to 
capture realizations of (posterior) rare classes.

There were at least two complications with the simulation. 
First, the classes in any LCA model are only identified up to
a permutation of the class labels. 
To wit, there are no distinguishable differences between 
estimates when say classes 1 and 2 are swapped in a model with 2+ classes.
The likelihoods are the same, the point estimates are likely to be
the same within numeric accuracy.
In any particular run of the \stcmd{gsem, lclass()} command, 
the classes depend first and foremost on the starting values. 
The help file 
\stcmd{help gsem\_estimation\_options}\verb|##|\stcmd{startvalues()}
outlines the possible options:

\begin{itemize}
  \item In my own practical work, I typically use 
    \stcmd{startvalues(randomid), draws(10)} or \stcmd{draws(20)}
    to get this many random assignments of the starting classes,
    and having Stata run the EM algorithm to get some decent 
    starting values for the gradient-based optimization methods.
  \item For the simulation purposes, you have to fix either 
    the initial assignment of classes, or the the starting values 
    of the estimates. The former is implementable with 
    \stcmd{startvalues(classid \textit{true\_class})} since 
    the latter is, of course, known. Strangely, in this simulation,
    this did not work out well as it resulted in convergence issues:
    it has been pushing the model into areas of the parameter space
    where the likelihood was too flat to climb out of.
  \item The resulting simulation specification I used in the simulation
    was a combination
    of \stcmd{from(b0) startvalues(jitter 0.1, draws(10))}. 
    The value of the starting matrix \stcmd{b0} was obtained
    from a sample of size $10^6$ computed once outside of the simulation.
    Jitter was added to allow some exploration of the sample optima 
    near that point.
\end{itemize}

The second complication of the simulation was that in some runs,
the \stcmd{mi estimate} calls with imputed classes were returning 
errors. 
There may be some minor incompatibilities 
between this third-party implementation of multiple imputation,
and the expectations of \stcmd{mi estimate}. Also, the discrete
nature of the imputed data may have played a role.
(The specific error message stated that an imputation had missing
data, and/or that sample sizes varied between imputations,
although visual inspection of the offending simulation runs 
showed this was not the case;
this may have been a somewhat generic error message produced by
the \stcmd{mi} engine when it encountered something unusual.)

\begin{stlog}
  \input{simul_failures.log.tex}\nullskip
\end{stlog}
  
  

With these limitations in mind, here are the simulation results.

Class probabilities are biased for the multiple imputation class prediction
methods (the population value for class 1 was 0.4.)

\begin{stlog} 
  \input{simul_class1pr.log.tex}\nullskip
\end{stlog}

The modal method exhibits greater variability
in class probabilities than the imputation methods.
The standard errors are severly biased down for all methods. 
Multiple imputation with more replicates has 
more stable standard errors.

\begin{stlog} 
  \input{simul_class1pr_se.log.tex}\nullskip
\end{stlog}

Turning to the outcomes in the model,
I picked two random summaries out of 15 
(=5 outcomes $\times$ 3 classes).
For the outcome $y_1$ in class 2, estimates are biased
away from the true value of 0.3 for the modal method:

\begin{stlog} 
  \input{simul_y1cl2pr.log.tex}\nullskip
\end{stlog}

The same observations concerning the standard errors apply: 
the variability of the modal method estimates is greater than 
the multiple imputation estimates, the standard errors are biased
for all methods, and the standard errors are 
more stable with the greater number of replicates.

\begin{stlog} 
  \input{simul_y1cl2pr_se.log.tex}\nullskip
\end{stlog}

Another outcome summarized, $y_4$ in class 1, has the target value
of 0.5. Biases are in opposite directions for the modal method
and multiple imputation methods. 

\begin{stlog} 
  \input{simul_y4cl1pr.log.tex}\nullskip
\end{stlog}

Problems with the variability
and its estimation with the standard errors persist.

\begin{stlog} 
  \input{simul_y4cl1pr_se.log.tex}\nullskip
\end{stlog}

Moving now to invetstigate the variable outside the model,
$y_6$, we observe that the estimates are severely biased
towards the pooled mean.

\begin{stlog} 
  \input{simul_y6cl1pr.log.tex}\nullskip
  \input{simul_y6cl3pr.log.tex}\nullskip
\end{stlog}

The modal method continues to exhibit greater variability
than the imputation methods.
For this outcome, however, the standard errors are much closer
to their targets. The standard errors of the modal method 
(mean of 0.058 for class 1) 
underestimate the true Monte Carlo variability (mean of 0.078), 
while the multiple imputation standard errors are biased upwards
(0.071 vs. the targets of 0.060).
Multiple imputation with more replicates has 
more stable standard errors.

\begin{stlog} 
  \input{simul_y6cl1pr_se.log.tex}\nullskip
\end{stlog}

Same observations apply to the standard errors for class 3.

\begin{stlog} 
  \input{simul_y6cl3pr_se.log.tex}\nullskip
\end{stlog}

Overall, it seems like the problem of the class indeterminacy
(permutation of classes) has still been damaging the simulation.
Further steps would need to be taken to better match the estimated
classes with their population counterparts.

\newpage

%%% to peek at the Stata Journal features
% discussion of the Stata Journal document class.
% \input sj.tex
% discussion of the Stata Press LaTeX package for Stata output.
% \input stata.tex

\bibliographystyle{sj}
\bibliography{post-lca}

\begin{aboutauthors}
Stas Kolenikov is Principal Statistician at NORC who has been
using Stata and writing Stata programs for about 25 years.
He had worked on economic welfare and inequality, spatiotemporal
environmental statistics, mixture models, missing data,
multiple imputation, structural equations with latent variables,
resampling methods, complex sampling designs, survey weights,
Bayesian mixed models, combining probability and non-probability samples,
latent class analysis, and likely some other stuff, too.
\end{aboutauthors}

\endinput
