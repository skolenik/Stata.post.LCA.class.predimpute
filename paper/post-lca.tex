\inserttype[st0001]{article}
\author{Kolenikov}{%
  Stas Kolenikov\\NORC\\Columbia, Missouri, USA\\kolenikov-stas@norc.org
}
\title[Post-estimation for LCA via MI]{Inference for imputed latent classes using multiple imputation}
\maketitle

\begin{abstract}
Latent class analysis (LCA) is a commonly used tool to perform confirmatory
analysis of multivariate relations between (typically) categorical variables 
based on modeling different low dimension tables in different latent classes.
The official Stata command for the task is \stcmd{gsem, lclass()}. 
It produces estimates of the class probabilities and the structure
of the manifest variable distributions (proportions in outcome categories).
Users of LCA are often interested in additional analysis, such as exploration
of classes with additional variables, or use of class membership
as a predictor variable in statistical models.
I introduce a new command \stcmd{postlca_class_predpute} to help with these tasks.
It uses the mechanics of multiple imputation (MI) to multiply impute
classes, sets up the necessary data structures and metadata for \stcmd{mi},
and allows users to run \stcmd{mi estimate} to produce descriptive statistics 
by latent class or fit the researcher's models of interest 
with appropriate corrections to the standard errors.
The long name of the command underlines that (1) it is a postestimation
command for LCA, (2) it deals with classes, and (3) it performs a combination
of class prediction and imputation.

\keywords{\inserttag, postlca\_class\_predpute, latent class analysis, multiple imputation}
\end{abstract}

\section{Introduction}

Latent class analysis (LCA) is a commonly used statistical and quantitative
social science technique of modeling counts in high dimensional contingency tables,
or tables of associations of categorical variables
\citep{hagenaars:mccutcheon:2002,heinen:1996,mccutcheon:1987,weller:bowen:faubert:2020}. 
It reduces the potentially complex multivariate relations
to a mixture of simple (e.g. unidimensional) relations.

Built-in Stata commands allow users to fit main effect latent class models
using \stcmd{gsem, lclass()} syntax. Users can produce a limited range 
of post-estimation analyses, including
estimates of class probabilities (which are modeled on the logit scale internally,
and thus require an \stcmd{nlcom} transformation) and class means/proportions of the outcome
variables (also modeled on the logit scale internally). 
Researchers are often interested in describing the latent classes
using additional variables external to the model,
or using classes in analysis as predictors or as moderators.
As these tasks 
break away from the estimation step in \stcmd{gsem, lclass()}, 
they typically fail to properly incorporate uncertainty in parameter
estimates and especially in class membership.

The command contributed by this package, 
\stcmd{postlca_class_predpute}, addresses the issue of permitting 
arbitrary analyses with latent classes based on the modeled classes.
(Well, almost ``arbitrary'': these analyses have to conform to Stata's
estimation standards and result in the estimation output 
including \stcmd{e(b)} vectors of estimates
and \stcmd{e(V)} variance-covariance matrix, so a handful of common
descriptive analyses such as computing correlations or producing 
cross-tabulations are still not feasible.)

The basic methodological foundations of LCA are introduced in Section \ref{sec:lca}.
It also provides the examples from the Stata manual, and explains how
the proposed new command fits into the ecosystem of latent modeling in Stata.
Section \ref{sec:new_command} provides the syntax, followed by examples
in Section \ref{sec:examples}. A simulation study is provided in Section \ref{sec:simul}.

\section{Latent class analysis}
\label{sec:lca}

Latent class analysis is a form of latent variable modeling where
both latent variables are discrete and the observed variables are discrete,
as well. Latent models with continuous outcomes and continuous latent variables
are often referred to as structural equation models (SEM) although that class
of models also contains systems of observed variables. Latent models
with continuous latent variables and discrete outcome variables
are often referred to as latent trait models. Item-response theory
models are examples of latent trait models.
Finally, latent models with continuous outcomes and discrete latent variables
are referred to as profile analysis.

\subsection{Log-linear models}

LCA with discrete data can be expressed as a form of loglinear modeling, 
a statistical method to model counts within cells defined by categorical variables.
Suppose the researcher has several categorical variables $X_1, X_2, \ldots, X_p$
with categories 1 through $m_j, j=1, \ldots, p$,
and counts $n_{{k_1}{k_2}\ldots{k_p}}$ 
in a complete $p$-dimensional table. 
Then the simplest main effects model for the counts is

\begin{equation}
  \label{eq:loglinear:main}
  n_{{k_1}{k_2}\ldots{k_p}} \sim \mathop{\rm Poisson} 
  \bigl(
    \mathrm{offset} + 
    \sum_{j=1}^p \sum_{k_j=1}^{m_j} \beta_{jk_j} 
  \bigr)
\end{equation}

\noindent
with applicable identification constraints (such as the sum of the coefficients
for each specific variable being set to zero, $\sum_{k_j} \beta_{jk_j} = 0$,
or the coefficient for the first category
of a variable being set to zero, $\beta_j1=0$). 
Those constraints are familiar to researchers who work with 
multinomial regression models where coefficients of a reference category
all need to be set to zero for identification.
Parameter estimates in (\ref{eq:loglinear:main}) can be obtained by maximum
likelihood.
This model can be denoted as $X_1 + X_2 + \ldots + X_p$ main effects model.

The fit of the model is assessed by the Pearson $\chi^2$ test comparing the
expected/predicted counts $\hat n_{{k_1}{k_2}\ldots{k_p}}$
vs. observed cell counts, or by the likelihood ratio test against a saturated
model where each cell in the full $p$-dimensional table
has its own coefficient. The Pearson test is typically found to be more
reliable, as the likelihood ratio test often runs into problem
of zero cells in high-dimensional settigs, where the corresponding
cell parameter is understood as undefined or $-\infty$, neither
of which is practicale.

In the loglinear model notation, the latent class models 
with main effects estimable by \stcmd{gsem, lcass()}
are models of the form $C \# (X_1 + X_2 + \ldots + X_p)$:
categorical latent variable $C$ is the latent class,
and all the outcome variables enter the model as main effects
conditional on the class,
or as two-way interactions with the latent class if the model
is expressed marginally:

\begin{equation}
  \label{eq:loglinear:lca}
  n_{{k_1}{k_2}\ldots{k_p}} | C \sim \mathop{\rm Poisson} 
  \bigl(
    \mathrm{offset}_C + 
    \sum_{j=1}^p \sum_{k_j=1}^{m_j} \beta_{Cjk_j} 
  \bigr),
  \quad
  \mathop{\rm Prob[C=c]} = \pi_c, \quad \sum_c \pi_c = 1
\end{equation}

The model is now a mixture of Poisson regressions,
and maximum likelihood estimation additionally involves 
estimating the prevalence $\pi_c$ of each class 
and integrating the likelihood over the classes.

Further extensions of latent class analysis may include:

\begin{enumerate}
  \item Analysis with interactions of the observed variables;
  \item Analysis with complex survey data (in which case estimation proceeds
      with \stcmd{svy} prefix, and the counts are the weighted estimates 
      of the population totals in cells);
  \item Constrained analyses with structural zeroes or ones 
      (e.g. that every member of class $C=1$ must have the value
      $X_1=1$, expressed as the corresponding coefficient 
      $\beta_{11\cdot}=-\infty$ for all categories except 1);
  \item Constrained analyses (measurement invariance) where some 
      variables or interations have identical
      coefficients across classes.
\end{enumerate}

\subsection{Official Stata implementation}

Official Stata \stcmd{gsem, lclass()} implements 
the \textit{main effects} LCA. The syntax is that of the SEM
families, with the variables that the arrow points to
interpreted as the outcome variables, and the latent class
variable not present in the arrow syntax
being considered the source of the arrow:

\begin{stlog}
. webuse gsem_lca1
\smallskip
. gsem (accident play insurance stock <- ), logit lclass(C 2)
\end{stlog}

The goodness of fit test against the free tabulation counts
is provided by \stcmd{estat gof} (not available after
the complex survey data analysis.)

As LCA is implemented through \stcmd{gsem},
all the link functions and generalized linear model families
are supported, extending the ``mainstream'' LCA to include
multinomial and ordinal variables. 

\subsection{Examples}

LCA has found use in analyses of complicated economic concepts
from survey data, and in assessment of measurement errors
that arise in the process.

\citet{biemer:2004} provided analysis of labor force classification
status (employed, unemployed, and out of labor force) following changes
in the survey instrument used in the Current Population Survey (CPS).
The latent classes are the true labor force participation
categories, and the observed variables are the corresponding 
survey measurements, demographics, and survey interview mode.
The model is a variation of LCA that accounts for the survey methodology
aspects of CPS: 
its panel nature (responses are collected over four consecutive months) 
and response mode (proxy reporting when a family member provides
the survey responses rather than the target person.)
He found that measurement of being employed and not being in labor force
are highly accurate (98\% and 97\% accuracy) while measurement of 
being unemployed is much less accurate (between 74\% and 81\% depending
on the analysis year.) LCA allowed to further attribute the drop in
accuracy of the unemployment status measurement to proxy reporting,
and to the problems with measuring the employment status when 
the worker is laid off.

\citet{kolenikov:daley:2017} analyzed the latent classes
of employees 
using the U.S. Department of Labor Worker Classification Survey.
The observed variables were (composite) 
self-report of the employment
status (are you an employee at your job; do you refer to your work
as your business, your client, your job, etc.); 
tax status
(the forms that the worker receives from their firm: W-2, 1099, K-1, etc.);
behavioral control 
(functions the worker performs and the degree of control over these functions,
such as direct reporting to somebody, schedule, permission to leave, etc.);
and non-control composite
(hired for fixed time or specific project). They found the best fitting
model to contain three classes: employees-and-they-know-it (59\%),
nonemployees-and-they-know-it (24\%), and confused (17\%) who
classify themselves as employees but their tax documentation
is unclear, and other variables tend to place them into non-employee status.

\subsection{Scope for this package}

The official \semref{gsem postestimation} commands provide
limited possibilities for LCA, namely reporting of the means
of the dependent variables by class via \stcmd{estat lcmean}.
For nearly all meaningful applications of LCA, this is insufficient.

The program distributed with the current package,
\stcmd{postlca\_class\_predpute}, provides a pathway for the appropriate
statistical inference that would account for uncertainty in class prediction.
This is achieved through the mechanics of multiple imputation (MI)
\citep{vanbuuren:2018:fimd2}. 
The approach was mentioned in passing by \citet{bolck:croon:hagenaars:2004}
who seem to have implemented it by hand.
The name of the command conveys that
\begin{enumerate}
  \item it is run after LCA as a post-estimation command;
  \item it predicts / imputes the latent classes.
\end{enumerate}

\subsection{Related work}

The \stcmd{step3} command \citep{repec:boc:bocode:s459182} implements
\citet{bolck:croon:hagenaars:2004} and \citet{vermunt:2010} analytic corrections
for prediction of the latent classes when the analysis proceeds
as follows.

\begin{enumerate}
    \item The measurement model only is fit (as in, e.g. \stcmd{gsem (y1 y2 y3 y4 <-), logit lclass(2)}).
    \item The classes are assigned based on the modal class probability.
    \item The multinomial regression model is fit to the classes as outcomes, with external predictors
          of class membership (as in, e.g. \stcmd{mlogit C_hat x1 x2 x3})
\end{enumerate}

These approaches were aimed at working around the limitations of the existing software
that was limited to fitting measurement models only at the time. With Stata LCA
capabilities, the whole process can be internalized with the joint estimation syntax

\begin{stlog}
. gsem (y1 y2 y3 y4 <- ) (C <- x1 x2 x3), logit lclass(C 2)
\end{stlog}

\noindent 
so there is no need for corrective steps to obtain consistent model
estimates and appropriate standard errors.

\section{The \stcmd{postlca_class_predpute} command}
\label{sec:new_command}

Following \stcmd{gsem, lclass()} estimation, users can obtain
multiply imputed latent classes using \stcmd{postlca_class_predpute} command
with the following syntax.

\begin{stsyntax}
    postlca\_class\_predpute,
    lcimpute(\varname)
    addm(\num)
    \optional{ seed(\num) }
\end{stsyntax}

\hangpara{
  \stcmd{lcimpute(}\varname\stcmd{)} 
  specifies the name of the latent class variable to be imputed.
  This option is required.
}

\hangpara{
  \stcmd{addm(}\num{)} specifies the number of imputations to be created.
  This option is required.
}

\hangpara{
  \stcmd{seed(}\num\stcmd{)} specifies the random number seed.
}

\section{Examples}
\label{sec:examples}

\subsection{Stata manual data set example}

The LCA capabilities of Stata are exemplified in [SEM] \textbf{Example 50g}:

\noindent
\begin{stlog}
\input{example_lca1_edited.log.tex}\nullskip
\end{stlog}

The official post-estimation commands available after
\stcmd{gsem, lclass()} produce the estimated class probabilities
and class-specific means of the outcome variables used in the model:

\noindent
\begin{stlog}
\input{example_lca1_lcamean.log.tex}\nullskip
\end{stlog}

The mutiple imputation version of this estimation task could
look as follows:

\begin{stlog}
\input{example_lca1_predpute.log.tex}\nullskip
\end{stlog}

The name of the latent class variable (here, \stcmd{lclass})
and the number of imputations are required. The seed is optional,
but of course is strongly recommended for reproducibility of the results,
as the underlying data are randomly simulated.
The multiple imputation version is notably faster.

As one of many diagnostic outputs of MI, the increase in variances / standard errors
due to imputations serves as an indication of how much of a problem
would treating the singly imputed (e.g. modal probability) latent classes would have been.
In the above output, the fraction of missing data (FMI)
is 33\% to 40\%, and the relative variance increase (RVI) is the similar range 
from 39\% to 45\%. This means that the analysis with the deterministic
(modal) imputation of the classes would have had standard errors 
that are about 20\% too small. (In fact, the model has a structural zero,
and modal imputation struggles to produce any standard errors around
the estimate, at all.)

\begin{stlog}
\input{example_lca1_modal.log.tex}\nullskip
\end{stlog}


\subsection{NHANES complex survey data example}

In many important and realistic applications of LCA, including the case
that necessitated the development of this package
\citep{rowan:etal:2024}, the data come from
complex survey designs that require setting the data up for the appropriate
survey-design adjusted analyses. See \svyref{svyset}, \miref{mi svyset},
and \citet{kolenikov:pitblado:2014}.

The standard data set for the \svyref{} commands is an extract from 
the National Health and Nutrition Examination Survey, Round Two
(NHANES II) data. I will use a handful of binary health outcomes
and one ordinal outcome to demonstrate LCA; the ordinal outcome
is arguably an extension that is not quite well covered in the
``classical'' social science LCA.

\begin{stlog}
\input{nhanes2.log.tex}\nullskip
\end{stlog}

This analysis approximates breaking down the population
into "generally healthy" and "unhealthy" groups, as e.g.
the gradient of \textit{hlthstat} variable between the classes
shows. 
Some complicated interaction of \stcmd{svy} and \stcmd{gsem} 
made the default starting values infeasible, 
hence I had to specify the initial random search.
The official \stcmd{gsem postestimation} commands
takes about 10 seconds for \stcmd{estat lcprob} 
and about 80 seconds for \stcmd{estat lcmean}.

The use of the \stcmd{postlca\_class\_predpute} command
makes it possible to run the post-estimation analylses much faster
(although that has never been the intent of the package),
and to conduct complementary analyses,
e.g. analysis of the racial composition of the two classes
using a variable that it outside of the model.

\begin{stlog}
\input{example_nhanes2_predpute.log}\nullskip
\end{stlog}

Imputation of 10 classes took less than a second.
The analogs of both \stcmd{estat lcprob} and
\stcmd{estat lcmean} took about 3 seconds to run.
(This time should be expected to be roughly linear
with the number of imputations $M$.)
As the new analysis of classes by race shows,
the less healthy class has a higher concentration
of ethnic minorities.

\subsection{Choosing the number of imputations}

One ``researcher's degrees of freedom'' aspect of this analysis
is the number of imputations $M$ that need to be created.
What this number affects the most is the stability of the standard
errors obtained through the multiple imputation process.
This stability is internally assessed with estimated
degrees of freedom associated with the variance estimate
\citep{barnard:rubin:1999}. With $M=10$ imputations,
the smaller ``poor health'' class have about 50 degrees of freedom:

\begin{stlog}
\input{example_nhanes2_dftable10.log}\nullskip
\end{stlog}

The public use version of the NHANES II data uses the approximate
design that has 62 PSU in 31 strata, resulting in 31 design degrees 
of freedom. The imputation degrees of freedom barely exceed that.
Let us push the number of imputations up (I used $M=62$ to match
the number of PSUs):

\begin{stlog}
\input{example_nhanes2_dftable62.log}\nullskip
\end{stlog}

The MI degrees of freedom are now comfortably above 600.
In many i.i.d. data situations, increasing the number of imputations
to several dozens can often send the MI degrees of freedom to 
approximate infinity (the reported numbers are in hundreds 
of thousands). With complex survey designs that have limited
degrees of freedom within each implicate, this may not materialize.
Researchers are encouraged to adopt the workflow where, in parallel,
they

\begin{enumerate}
  \item start with a small number of imputations, like \stcmd{addm(10)}
        in the example above, and develop the analysis code for all
        the substantive analyses, and
  \item working with the key outcomes or analyses, 
        experiment with several values of $M$ to find a reasonable
        trade-off when degrees of freedom exceed the sample size 
        for i.i.d. data, and/or exceed the design degrees of freedom
        for complex survey data by a factor of 3--5.
\end{enumerate}

Then a chosen value of $M$ can be used for all analyses in 
the manuscript under preparation.

An additional consideration for the choice of the number of imputations
is ensuring diversity of the simulated classes.
Even a large number of replications may not protect the researcher
from classes that may have structural zeroes. These produce
zero standard errors and missing degrees of freedom and 
variance increase statistics:

\begin{stlog}
\input{example_nhanes2_dftable62gh.log}\nullskip
\end{stlog}

Zero cases of poor health in the ``healthier'' class 
could be considered a structural zero. If information
on structural zeroes is available from substantive considerations,
it may be incorporated through constraints at estimation stage.

\section{Simulation}
\label{sec:simul}

So how much of a problem is the modal imputation of the latent classes?
I ran a small simulation to investigate. Samples were taken from
a model with three classes, five binary indicators and one additional 
continuous variable not used in the model as follows:

\begin{tabular}{l|ccc}
   \noalign{\smallskip}
  \hline
  \noalign{\smallskip}
  Variable & Class 1 & Class 2 & Class 3 \\
  \noalign{\smallskip}
  \hline
  \noalign{\smallskip}
  Class probability & 0.4 & 0.4 & 0.2 \\
  $\mathbb{P}[y_1=1|C]$ & 0.7 & 0.3 & 0.6 \\
  $\mathbb{P}[y_2=1|C]$ & 0.8 & 0.5 & 0.6 \\
  $\mathbb{P}[y_3=1|C]$ & 0.5 & 0.4 & 0.7 \\
  $\mathbb{P}[y_4=1|C]$ & 0.5 & 0.3 & 0.7 \\
  $\mathbb{P}[y_5=1|C]$ & 0.8 & 0.4 & 0.3 \\
  \noalign{\smallskip}
  \hline
  \noalign{\smallskip}  
  $y_6\sim N(\mu_c,1)$ & 1 & $\sqrt{2}=1.41$ & $\sqrt{3}=1.73$ \\
  \noalign{\smallskip}
  \hline
  \noalign{\smallskip}  
\end{tabular}

The LCA model with outcomes $y_1$ through $y_5$ was estimated,
and the classes were predicted using the posterior modal prediction,
multiple imputation with $M=5$ imputed data sets, and $M=50$ imputed 
data sets.
(To be precise, there was a single imputation with $M=50$, but 
two versions were run: \stcmd{mi estimate , imp(1/5)} for the limited
application with $M=5$, and without the \stcmd{imp()} option for the full
set of $M=50$.) Inspecting the individual runs visually, a shorter set
of imputations often results in insufficient detail, namely failing to 
capture realizations of (posterior) rare classes.

There were at least two complications with the simulation. 
First, the classes in any LCA model are only identified up to
a permutation of the class labels. 
To wit, there are no distinguishable differences between 
estimates when say classes 1 and 2 are swapped in a model with 2+ classes.
The likelihoods are the same, the point estimates are likely to be
the same within numeric accuracy.
In any particular run of the \stcmd{gsem, lclass()} command, 
the classes depend first and foremost on the starting values. 
The help file 
\stcmd{help gsem\_estimation\_options}\verb|##|\stcmd{startvalues()}
outlines the possible options:

\begin{itemize}
  \item In my own practical work, I typically use 
    \stcmd{startvalues(randomid), draws(10)} or \stcmd{draws(20)}
    to get this many random assignments of the starting classes,
    and having Stata run the EM algorithm to get some decent 
    starting values for the gradient-based optimization methods.
  \item For the simulation purposes, you have to fix either 
    the initial assignment of classes, or the the starting values 
    of the estimates. The former is implementable with 
    \stcmd{startvalues(classid \textit{true\_class})} since 
    the latter is, of course, known. Strangely, in this simulation,
    this did not work out well as it resulted in convergence issues:
    it has been pushing the model into areas of the parameter space
    where the likelihood was too flat to climb out of.
  \item The resulting simulation specification I used in the simulation
    was a combination
    of \stcmd{from(b0) startvalues(jitter 0.1, draws(10))}. 
    The value of the starting matrix \stcmd{b0} was obtained
    from a sample of size $10^6$ computed once outside of the simulation.
    Jitter was added to allow some exploration of the sample optima 
    near that point.
\end{itemize}

The second complication of the simulation was that in some runs,
the \stcmd{mi estimate} calls with imputed classes were returning 
errors. 
There may be some minor incompatibilities 
between this third-party implementation of multiple imputation,
and the expectations of \stcmd{mi estimate}. Also, the discrete
nature of the imputed data may have played a role.
(The specific error message stated that an imputation had missing
data, and/or that sample sizes varied between imputations,
although visual inspection of the offending simulation runs 
showed this was not the case;
this may have been a somewhat generic error message produced by
the \stcmd{mi} engine when it encountered something unusual.)

\begin{stlog}
  \input{simul_failures.log.tex}\nullskip
\end{stlog}
  
  

With these limitations in mind, here are the simulation results.

Class probabilities are biased for the multiple imputation class prediction
methods (the population value for class 1 was 0.4.)

\begin{stlog} 
  \input{simul_class1pr.log.tex}\nullskip
\end{stlog}

The modal method exhibits greater variability
in class probabilities than the imputation methods.
The standard errors are severly biased down for all methods. 
Multiple imputation with more replicates has 
more stable standard errors.

\begin{stlog} 
  \input{simul_class1pr_se.log.tex}\nullskip
\end{stlog}

Turning to the outcomes in the model,
I picked two random summaries out of 15 
(=5 outcomes $\times$ 3 classes).
For the outcome $y_1$ in class 2, estimates are biased
away from the true value of 0.3 for the modal method:

\begin{stlog} 
  \input{simul_y1cl2pr.log.tex}\nullskip
\end{stlog}

The same observations concerning the standard errors apply: 
the variability of the modal method estimates is greater than 
the multiple imputation estimates, the standard errors are biased
for all methods, and the standard errors are 
more stable with the greater number of replicates.

\begin{stlog} 
  \input{simul_y1cl2pr_se.log.tex}\nullskip
\end{stlog}

Another outcome summarized, $y_4$ in class 1, has the target value
of 0.5. Biases are in opposite directions for the modal method
and multiple imputation methods. 

\begin{stlog} 
  \input{simul_y4cl1pr.log.tex}\nullskip
\end{stlog}

Problems with the variability
and its estimation with the standard errors persist.

\begin{stlog} 
  \input{simul_y4cl1pr_se.log.tex}\nullskip
\end{stlog}

Moving now to invetstigate the variable outside the model,
$y_6$, we observe that the estimates are severely biased
towards the pooled mean.

\begin{stlog} 
  \input{simul_y6cl1pr.log.tex}\nullskip
  \input{simul_y6cl3pr.log.tex}\nullskip
\end{stlog}

The modal method continues to exhibit greater variability
than the imputation methods.
For this outcome, however, the standard errors are much closer
to their targets. The standard errors of the modal method 
(mean of 0.058 for class 1) 
underestimate the true Monte Carlo variability (mean of 0.078), 
while the multiple imputation standard errors are biased upwards
(0.071 vs. the targets of 0.060).
Multiple imputation with more replicates has 
more stable standard errors.

\begin{stlog} 
  \input{simul_y6cl1pr_se.log.tex}\nullskip
\end{stlog}

Same observations apply to the standard errors for class 3.

\begin{stlog} 
  \input{simul_y6cl3pr_se.log.tex}\nullskip
\end{stlog}

Overall, it seems like the problem of the class indeterminacy
(permutation of classes) has still been damaging the simulation.
Further steps would need to be taken to better match the estimated
classes with their population counterparts.

\section{Conclusions}

This article proposed a new post-estimation command 
\stcmd{postlca_class_predpute} to facilitate analyses that
want to utilize classes predicted after the \stcmd{gsem, lclass()}
latent class modeling. The article provided methodological
justification for the command, the syntax, and presented 
a simulation study that underscored some of the issues
that practitioners of LCA should have in mind when
applying both the official Stata modeling capabilities,
and the new command.

\newpage

%%% to peek at the Stata Journal features
% discussion of the Stata Journal document class.
% \input sj.tex
% discussion of the Stata Press LaTeX package for Stata output.
% \input stata.tex

\bibliographystyle{sj}
\bibliography{post-lca}

\begin{aboutauthors}
Stas Kolenikov is Principal Statistician at NORC who has been
using Stata and writing Stata programs for about 25 years.
He had worked on economic welfare and inequality, spatiotemporal
environmental statistics, mixture models, missing data,
multiple imputation, structural equations with latent variables,
resampling methods, complex sampling designs, survey weights,
Bayesian mixed models, combining probability and non-probability samples,
latent class analysis, and likely some other stuff, too.
\end{aboutauthors}

\endinput
